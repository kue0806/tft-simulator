TFT 강화학습에서 Self-Play를 구현해줘.
목표
에이전트가 자기 자신과 대결하면서 점점 강해지는 Self-Play 학습 시스템을 만들어줘. 최종 목표는 8명 플레이어 환경에서 강한 에이전트를 만드는 거야.

1단계: Self-Play 환경 구현
src/rl/env/self_play_env.py 파일을 만들어줘.
기존 TFTEnv를 확장해서, 봇 플레이어들도 학습된 에이전트 정책을 사용하게 해줘. 즉, 8명 플레이어가 모두 같은 (또는 과거 버전) 에이전트로 플레이하는 거야.
환경은 한 명의 에이전트 시점에서 동작하지만, 다른 7명도 동일한 신경망으로 행동을 선택해야 해. 매 스텝마다 현재 에이전트가 행동하고, 다른 플레이어들도 각자의 관찰을 받아서 행동해야 해.

2단계: 에이전트 풀 관리
src/rl/training/agent_pool.py 파일을 만들어줘.
학습 중에 주기적으로 현재 에이전트를 저장해서 "과거 버전 풀"을 관리해야 해. 풀에는 최대 10~20개의 과거 버전을 유지해줘.
대전 상대를 선택할 때는 이 풀에서 랜덤하게 뽑아서 사용해. 이렇게 하면 학습이 안정적이고, 특정 전략에만 최적화되는 걸 방지할 수 있어.
저장 주기는 5만 스텝마다 한 번씩 해줘.

3단계: Curriculum 설정
학습 초반에는 약한 상대와 섞어서 학습하고, 점점 자기 자신과만 대결하게 해줘.

0~20만 스텝: 상대 중 50%는 랜덤 봇, 50%는 Self-Play
20~50만 스텝: 상대 중 25%는 랜덤 봇, 75%는 Self-Play
50만 스텝 이후: 100% Self-Play (과거 버전 풀에서 선택)

랜덤 봇은 기존처럼 하되, 유닛을 보드에 배치하도록 수정해줘. 구매한 유닛을 빈 보드 칸에 자동으로 배치하는 로직을 추가해.

4단계: 학습 파이프라인
src/rl/training/self_play_trainer.py 파일을 만들어줘.
학습 설정:

총 스텝: 200만
병렬 환경: 8개
학습률: 1e-4
에이전트 풀 저장 주기: 5만 스텝
Entropy coefficient: 0.02 (탐색 유지)

매 10만 스텝마다 현재 에이전트를 과거 버전들과 100 에피소드 대결시켜서 승률을 측정해줘. 이걸 ELO처럼 추적하면 좋아.

5단계: 평가 시스템
학습된 에이전트의 강도를 측정하기 위해:

랜덤 봇 7명 상대 승률 (baseline)
과거 버전들 상대 승률 (진짜 실력)
세대별 ELO 레이팅 추정

100 에피소드씩 평가하고, 평균 순위와 Top 4 비율을 보고해줘.

6단계: 학습 안정화
학습이 불안정하거나 성능이 떨어지면:

과거 버전 풀에서 "강한 버전"만 유지하도록 필터링해줘
현재 에이전트가 풀의 평균 승률 40% 이하면 학습률을 낮추거나 롤백해줘
Entropy coefficient를 높여서 탐색을 늘려줘


결과 보고
학습 완료 후 다음을 보고해줘:

세대별 ELO 또는 승률 변화 그래프
최종 에이전트의 랜덤 봇 상대 Top 4 비율
최종 에이전트의 과거 버전 상대 평균 순위
가장 많이 사용하는 전략 패턴 (있다면)


성공 기준

랜덤 봇 상대 Top 4 비율 90% 이상
10세대 전 버전 상대 승률 60% 이상 (즉, 계속 강해지고 있음)


파일 구조 요약
src/rl/
├── env/
│   ├── tft_env.py           # 기존
│   └── self_play_env.py     # 새로 만들기
│
├── training/
│   ├── trainer.py           # 기존
│   ├── agent_pool.py        # 새로 만들기
│   └── self_play_trainer.py # 새로 만들기
테스트도 작성해서 전체 테스트가 통과하는지 확인해줘.재시도